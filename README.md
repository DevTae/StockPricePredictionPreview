### 주식 가격 예측 모델 개발 일지

<br/>

### 2023.7.24 - 2023.8.3

#### 진행사항
  - `주가 데이터 수집`(보조지표 포함)과 metadata 형식으로 `전처리 과정` 진행
  - `주가 분석 모델 툴킷 제작` 및 `모델 설계` (CNN + RNN 기반) 진행
  - `하이퍼 파라미터 설정` 및 학습 진행

<br/>

### 2023.8.6

#### 현재 문제점
  - 70~100 epochs 를 최종 목표로 하고 있는 도중, 10, 20, 30 epochs 일 때의 Training 과 Validation 에 대한 MSE 와 Loss 를 보면서 모델 품질에 대하여 판단하는 중이다.
  - 그 결과, Training 에 대해서는 MSE 가 대폭 줄어들고 있지만, Validation 에 대해서는 MSE 가 줄고 있지 않는 `과적합 상태`를 발견하였다.

#### 예상되는 원인
  - 현재 metadata.txt 에서 시장과 종목 코드를 기준으로 정렬하게 해두었는데, 이로 인하여 Validation 데이터셋 쪽에 변동폭이 큰 종목들이 대거 들어가게 되어 그렇게 된 것 같다.

#### 데이터에 대한 고찰 (23.8.6)

- 가격 움직임에 대하여 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율이 비슷한가?
  - 변동성에 대한 지표를 바탕으로 변동성이 큰 종목과 변동성이 작은 종목에 대한 비율을 조사한다. 이것도 히스토그램으로 얼마나 분포되어 있는지 확인할 수 있어야 함.
  
- 수익률이 기존 가격에 근접한 종목과 큰 수익을 달성하는 종목의 비율이 어떻게 구성되어 있나?
  - 히스토그램을 바탕으로 수익에 있어서 10% 단위에 대하여 데이터의 분산을 확인한다. 
  - 만약, 비율이 다르다면 어떻게 진행하는 것이 좋을 것 같나?
  - 일단, train 과 validation 데이터를 골고루 분산시키는 것이 좋겠다. metadata.txt 에 있는 데이터들을 기반으로 각 데이터들에 대하여 sort 를 진행하고 만약 train 과 validation 사이의 관계가 10:1 이라면 (전체데이터개수)/11 (=x) 을 계산하여 10x 를 train 데이터에 넣고, x 를 validation 데이터에 집어넣는 방식으로 진행.
  - 비교적 변동성이 큰 종목의 데이터 개수가 모자랄 것이라는 생각이 든다. 이에 대하여 데이터 개수가 부족한 부분에 대하여 데이터 증강을 통하여 접근할 수 있도록하면 좋겠다는 생각을 하였다.
  
- 데이터 증강의 경우, 정확하게 어떤 방식을 선택할 것인가?
  - 현재, Time Domain 에 있어서 기본적으로 제안하는 증강 기법은 Window cropping or slicing, Window warping, Flipping, Noise injection, Label expansion 정도가 있다. 나에게 적합한 방식은 Noise Injection 가 될 것 같다. 노이즈로는 gaussian noise, spike, step-like trend, and slope-like trend 정도가 있다고 한다. 최종적으로 gaussian noise 를 적용할 예정이다.
  - 당장에는 시계열 데이터의 증강을 적용하므로써 생길 데이터의 변질이 우려되어 적용하고 있지 않는 중이다.

- **결론**
  - `random.shuffle` 함수를 바탕으로 **변동성이 큰 종목이 몰려 있는 validation 데이터셋**과 **training 데이터셋**을 섞는 것을 목표로 하였음.
  - 위 방법을 통하여 Training Set 과 Validation Set 의 MSE 가 엄청나게 많이 차이나는 현상을 해결할 수 있었음.

<br/>

### 2023.8.10

#### EDA 접근
- 결과층 데이터가 `특정 구간에 몰려 있는 것`을 확인 / 이러한 부분이 `데이터 불균형`을 뜻하고 `학습 평가 검증`이 다소 어려워질 수 있음
- 따라서, `결과값 확대 및 축소 함수(log 및 sqrt 공식)`를 적용하여 다음과 같이 `정규 분포 형태의 데이터 분포`로 구성할 수 있었다.
  
  ![화면 캡처 2023-08-13 031115](https://github.com/DevTae/StockDatabasePreview/assets/55177359/b9340afa-88bd-4cf8-b92b-9c0c3dbb580b)
- 데이터 분포의 문제를 해결할 수 있어 당장에는 Data Augmentation 을 적용하지 않고 학습을 진행하고자 한다.
  - 데이터 수는 약 160만 개로 충분히 커버가 가능할 듯 보인다.

<br/>

### 2023.08.19

#### 모델 구조 변경
  - `CNN + RNN` 결합한 모델을 바탕으로 학습을 돌려본 결과, 생각보다 좋은 성능이 나오질 않았다.
    - `평균 약 5~6% 내외`의 오차율을 보임 / `MSE 1000`, `MAE 28`
      - 평가지표의 경우, 결과층에 의도적으로 Scaling 을 진행하지 않아 이런 결과가 나오게 되었음. (뒤에 모두 비슷한 스케일의 검증 결과가 나올 예정임)
  - 이후, `LSTM` 을 바탕으로 `Layer Normalization 기법`까지 적용하여 모델을 구성하였고 학습을 시작할 수 있었다.

<br/>

### 2023.08.20

#### 하이퍼파라미터 튜닝 및 전처리 스케일링 방식 변경
  - 모델 구조를 바꿨음에도 `MSE 가 1000 부근`으로 예상보다 높게 나옴으로 `하이퍼파라미터 설정`과 `스케일링 방식`을 변경하기로 하였다.
  - 학습률, Epoch, RNN 레이어 개수, Hidden Dimension 에 대한 하이퍼파라미터 튜닝을 진행하였다.
  - 마지막 종가를 바탕으로 모든 수치를 계산하지 않고 `전날 종가를 기준으로 수치를 스케일링`하여 변환하였다.
  
<br/>

### 2023.08.21 ~ 2023.08.22

#### 현재 학습 결과
  - 새롭게 **결과층에 변환된 값을 해제**하는 공식을 적용하여 `PriceMeanSquaredError` 평가지표를 바탕으로 `PMSE 가 39.19` 정도가 나왔다.
    - 20 Epochs 에 `PMSE 가 39.19` (이전 MSE 와 기준이 다름) 정도가 나오며 평균적으로 `약 6.2% 의 오차`를 가지고 있다는 것을 알 수 있다.
      - (23.8.25) 위의 평균 6.2% 의 오차라는 것은 많이 `편향(bias)`된 검증 결과이다. 따라서, `데이터 분포 변환의 필요성`을 느끼게 됨.
  - 이전의 CNN + RNN 결합한 모델과 그렇게 큰 차이를 보이진 않는 것으로 보여, `GRU 모델`을 바탕으로 진행하고자 한다.
  
#### 이후 방향성
  - 스케일링 방식을 바탕으로 Feature 들에 대하여 `Non-Stationary 데이터`가 되도록 변경할 것이다.
  - `하이퍼 파라미터 튜닝`을 계속하여 진행할 것이다.

<br/>

### 2023.08.23 ~ 2023.08.25

#### 현재 진행 상황
  - 스케일링 방식을 변형하여 모든 데이터들에 대하여 `Non-Stationary 데이터`로 변환하였음.
  - 이전 데이터셋은 모양만 정규분포와 유사하였고, `Shapiro-Wilk 검정`을 진행한 결과, 정규분포가 아님을 확인하여 이에 대한 데이터 선정 알고리즘을 적용하고자 함.
    - 추가적으로, 정규분포보다 `균등분포 (Uniform Distribution)`에 더 가깝도록 데이터를 설계하고자 하였고 `일련의 보정`을 진행하였음.
      - 결과층의 반환값이 편향된 분포를 가지면 안 되는 이유로 데이터를 선정하였고 고르게 분포하도록 했기 때문에 이전 데이터셋의 검증 결과와 비교할 수 없음.
    - `QQ Plot` 을 진행한 결과, 다음과 같이 균등하게 분포하도록 개선할 수 있었음.
      
      ![image](https://github.com/DevTae/StockDatabasePreview/assets/55177359/a66c4bc9-f106-4cb1-b83a-c44932a73f2d)
  - 전처리 방식 변경
    - 출력층 기준 변경 (look-ahead period)
    - 데이터 선정 알고리즘 이외에도 `변동성 차원에서의 지표 조건을 추가`하여 데이터 선정 (도메인 지식 적용)

<br/>

### 2023.8.26

#### 현재 진행 상황
  - 하이퍼 파라미터 설정 진행 중에 `warmup steps` 에 관련된 실험을 진행 중이다.
    - `warmup steps` 를 전체 step size (모든 epochs) 의 **약 10% 가량**으로 설정한 것과 그것보다 훨씬 적게 **약 1% 가량**으로 설정하는 것과의 차이를 비교해보았다.
    - 그 결과, `후자의 경우`가 MSE 에 있어서 비교적 빠르게 수렴하는 것을 확인할 수 있었다.
    - 예상되는 이유로는 `warmup steps` 가 많을 경우, 초반에 빠르게 학습하지 못하여 이후 전체 학습이 느려지는 이유인 것 같고, 추가적으로 초반에 `local minima` 가 발생할 수 있다는 점 때문이다.
    - 따라서, 빠른 수렴 이후 안정성까지 어느 정도 조화를 이루는 적절한 `warmup steps` 수치를 정하는 것이 좋겠다.
    - 결과적으로는 **전체 epoch 에 대한 step size 의 약 1% 가량**으로 설정 후 진행하고자 한다.

<br/>

### 2023.8.29

#### 현재 진행 상황
  - 데이터 수치가 `lower boundary` 와 `upper boundary` 를 벗어나는 경우, 제외하지 않고 `clipping` 하는 방식으로 데이터를 보존하였다.
    - 균등 분포를 만드는 과정에서 데이터를 제외하는 이유로 전체 데이터 개수가 부족해지는 현상이 생기기 때문에 위 방식을 적용하게 되었다.
    - 이상치 데이터 개수가 적기 때문에 약간의 데이터를 살리는 것만 해도 상당한 비율의 데이터를 구하게 되는 것이다.
    ![image](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/0c6ffa7c-b1a0-415c-bccc-05941c134409)
  - 출력층 기준 변경 (look-back period, look-ahead period)
  - 출력층 산정 방식 변경 (수식을 바탕으로 상승 확률로 표현)
  - 학습 도중에 생기는 `멈춤 현상` 을 해결하기 위하여 `해시 함수`를 활용하여 `Disk I/O` 를 줄이고자 한다.
    - 현재, 데이터 추출 방식을 동일한 raw data 에서 반복적으로 읽는 방법으로 진행하고 있는데, 이러한 구조 때문인지 학습 도중 1 step 이 이전의 1000 배의 시간이 걸리는 현상이 발생하었다. 그 이후, 프로그램이 종료되었다.
    - `num_workers` 가 4 로 설정되어 있었고, 디버깅해보았을 때 `CUDA Out of Memory` 등의 오류가 발생하지 않았다는 점에서 동일한 파일에 대한 여러 번의 접근으로 인한 `Deadlock` 발생에 대한 의심을 하는 중이다.
    - 이러한 상황에서 가장 먼저 해볼 목표는 `해시 함수`를 활용하여 `Disk I/O` 를 줄이는 것이다.

      ![image](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/598338d9-e678-49ef-945d-8c0a9a79b39a)
    - 그 결과, 성공적으로 다음과 같이 초반에 데이터를 읽어들인 이후에는 메모리에서 가져오기 때문에 성공적으로 `Disk I/O` 를 줄일 수 있었고, 그에 따라 평균 소요 시간도 `약 75% 가량 절감`할 수 있었다.
  - 위와 같이 `Disk I/O` 를 줄였음에도 학습 진행 중 똑같이 `멈춤 현상`이 발생하였고, 추가적으로 `Memory Leak` 현상이 의심되어 매 반복문에서 할당되는 `torch`, `cpu` 변수를 해제하도록 하였다.
  - 앞으로는 하이퍼파라미터 튜닝을 진행하며 라이브러리인 `optuna` 또한 학습해볼 예정이다.
  - 현재 부정부터 긍정까지 `0 ~ 100` 사이에서 표현(*서비스 상에서*)하도록 하였고, `평균 오차 목표`를 `전체의 10%` 정도로 잡았으며, 이에 따라 `PMSE` 가 `25` 정도가 되는 것이 목표이다.
    - (23.8.31) 현재, `-1 ~ 1` 사이의 출력 목표 기준으로 `0.1` 정도의 오차를 허용하며 `최종 목표 MSE` 는 `0.01` 로 재설정하였다.

<br/>

### 2023.8.31

#### 현재 진행 상황
  - `torch` 의 `to` 메소드에 대하여, 원래의 `torch` 값이 소실되는 우려를 바탕으로 `clone` 으로 `cpu` 에 복제 후 평가지표 계산하는 데에 쓰이도록 변경하였다.
    - (23.9.2) 해당 부분에 대하여, `torch` 값을 `cpu` 에 복제하지 않은 상태에서 사용하고 `del` 로 초기화해주는 방식으로 변경하였다.
  - 학습 중 Loss 가 줄어들지 않고 있는 상황임을 발견하였다.
    - 모델 구조 변경 시도 (`Transpose`, `Dropout`, `Linear`, `Xavier Uniform`, `RReLU` 등 다방면으로 적용 시도)
    - Scaling 기준 변경 (입력값은 `-1 ~ 1` 로 스케일링하였고, 출력값은 `0 ~ 1` 사이로 스케일링 진행)
      - 따라서, `목표 MSE` 는 출력층 데이터에서의 `0.1 정도 오차`이기 때문에 `0.01` 로 재설정
  - 위 시도를 통하여, `Loss 가 수렴`하는 것을 확인하여 `발산에 대한 문제를 해결`할 수 있었다.

<br/>

### 2023.9.1

#### 현재 진행 상황
  - 이전에 학습 중 생긴 `멈춤 현상`이 `Deadlock` 때문이라고 판단하였었지만 추가적인 원인으로 `CPU 할당률`이 100% 가 넘는 것을 의심하였고 설정 변경을 시도하였다.
    - `num_threads` 를 `4 -> 1` 으로 수정하였고, CPU 할당률을 자동으로 조절할 수 있도록 하였다.
  - 위 방법들을 확인해보았지만 문제는 해결되지 않았고, `Linux` 의 `dmesg -T --level err` 명령어를 바탕으로 에러에 대한 원인을 찾을 수 있었다.

    ![화면 캡처 2023-09-01 235603](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/2e18b98a-8ae3-4a3a-b618-eba4ee8e7e85)
    - 결국에는 `Out of memory` 및 `Memory Leaking` 으로 인하여 생긴 문제였고, 이에 대하여 `메모리 최적화`, `스왑 공간 확인` 등의 방법을 바탕으로 해결할 예정이다.
  - 종료 원인을 찾기 위하여 `psutil` 라이브러리를 통한 `메모리 사용량 디버깅`을 진행하고자 한다.
    - 로그 출력에 메모리 사용량(Memory Usage) 또한 출력하게 하여 확인해본 결과, 메모리 사용량이 계속해서 늘어나는 현상이 발견되었다.

      ![image](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/a34ba749-e967-448a-a7f1-320e3cb2a906)
    - 해당 현상에 대한 원인을 계속해서 찾고자 한다.

<br/>

### 2023.9.2

#### 현재 진행 상황
  - 평가지표를 `MSE` 에서 `MAE` 로 변경하였다.
    - 따라서, MAE 목표치는 `0.1` 로 정리 가능하다.
  - 캐시 적용 유무에 따라 메모리 변동이 다르기에 `각 100 steps` 별로 `캐시를 적용한 것`과 `적용하지 않은 것` 사이에서 비교(`psutil` 라이브러리 활용)를 진행해보았다.

    ![제목 없음](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/05cd8c98-b0b6-4b9d-af6f-654bcd791efd)
    - 그 결과, `캐시를 적용한 것`은 `Memory Usage` 측면에서 처음부터 계속 상승하고, `캐시를 적용하지 않은 것`은 처음엔 상승하지 않는 것처럼 보이다가 중간부터 계속해서 늘어나는 것을 볼 수가 있었다.
    - 결국에는, `Memory Leaking` 현상이 일어났다는 것이고 조금 더 세밀한 실험이 필요했다.
  - 그에 따라, 메모리 동향을 `guppy3` 라이브러리를 통하여 `각 100 steps` 별로 확인해보았다.
  
    ![23 9 2](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/d2ac1aa0-d4a4-4d42-bdee-caf0f031b037)
    - 중간 단계에서부터 `list`, `str`, `tuple`, `int`, `float` 등의 변수들을 바탕으로 산정해보았을 때, 별다른 메모리의 변동은 없었다.
    - 하지만, CPU 점유 메모리는 계속해서 늘어나는 상황이었다.
  - 결과적으로, 위 단서를 바탕으로 생각해볼 수 있는 가설은 다음과 같았다.
    - `guppy3` 는 `torch` 변수에 대한 크기를 반영하지 않기 때문에, 어디에선가의 `torch` 변수에 의해 `Memory Leaking` 현상이 일어나고 있다.
    - `Garbage Collector (GC)` 가 정상 작동하지 않고 있다.
  - 계속해서 문제를 해결하기 위해 추가적으로 조사할 예정이다.

<br/>

### 2023.09.03

#### 현재 진행 상황
  - `Memory Leaking` 현상의 원인을 찾을 때 `tracemalloc` 라이브러리가 어떤 라인에서 메모리가 쌓이는지 볼 수 있다고 하여 해당 라이브러리를 사용하여 문제를 해결하고 있는 중이다.
    - 우선, 해시 테이블 캐시를 제거한 상태에서의 `Memory Leaking` 현상을 분석해보았다.

    ![2023-09-04 metrics 문제 확인](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/f0dbfb02-3307-4206-b7d3-e65e03f3a860)
    - 그 결과, 평가 지표를 계산하는 부분의 `abs` 함수에서 메모리가 누수되고 있는 현상을 발견하였다.
    - 해당 부분을 파이썬의 `삼항 연산자`로 대체하였고, 평가 지표 계산하는 부분의 메모리 누수를 해결할 수 있었다.
      - `dist = (target[0] - y_hat[0] if target[0] > y_hat[0] else y_hat[0] - target[0])`
      - 캐시 기능 없이는 `Memory Leaking` 현상 없이 정상적으로 동작하고 있다.
      - 다시 캐시를 적용한 뒤에 학습을 돌렸을 때 아직도 `Memory Leaking` 현상이 의심되는 `Memory Usage` 동향이 발견되었다.
      - 캐시 적용으로 `75%` 가 넘는 학습 시간을 단축할 수 있기에 해당 문제를 해결하고자 한다.
    - 이에 따른 원인을 계속해서 분석하고 해결할 예정이다.
   
<br/>

### 2023.09.04

#### 현재 진행 상황
  - `tracemalloc` 으로 확인해보았을 때, `abs` 에 대한 메모리 누수를 해결한 후부터 아래 그림처럼 눈에 띄는 메모리 누수의 경우 발견되지 않았다.

    ![문제 없음 발견](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/fb440ed8-11ee-45ec-95a1-01ed7a48ef67)
  - 하지만, `garbage collector` 라이브러리를 통하여 직접 확인해보았을 때, 아래 그림처럼 가비지 콜렉터의 `2 세대` 에 대한 메모리가 누적됨을 발견하였다.

    ![gc 작동 중 2세대 계속 증가](https://github.com/DevTae/StockPricePredictionPreview/assets/55177359/467c4508-c13b-4fb2-a420-05cf53328ce2)
  
<br/>

### 2023.09.19

#### 현재 진행 상황
  - `캐시 유무에 따른 메모리 동향`을 재확인하고자 캐시 사용 여부를 설정으로 변경할 수 있게 하고 일단은 `캐시를 비활성화`하여 학습을 시작하였음.
  - 데이터 학습할 때, `액면분할`, `액면병합` 등에 의해서 `액면가`가 변화하는 것이 반영되지 않은 데이터의 경우에 대한 예외처리를 진행하지 않는 것을 발견하여 이에 대한 예외처리를 진행하였다.

<br/>
